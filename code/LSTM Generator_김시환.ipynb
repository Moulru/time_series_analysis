{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NoPPXrPovT_"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.11.0\n",
    "# !pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgmO6vQr2zuF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLnpR2J92rm0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVIoF-lGrLPh"
   },
   "source": [
    "# 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjWfqNUDovgz"
   },
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, target_size=(256, 256)):\n",
    "    paths = sorted(glob.glob(os.path.join(folder, '*.png')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path).convert('L').resize(target_size)  # 이미지를 그레이스케일로 변환하고 크기 조정\n",
    "        img_array = np.array(img) / 255.0 # 0-1 범위로 정규화\n",
    "        images.append(img_array[..., np.newaxis])  # (128,128,1) # 채널 차원 추가 (128,128,1)\n",
    "    return np.array(images)\n",
    "\n",
    "image_folder = \"drive/MyDrive/Colab Notebooks/Project2/Model/img256\"\n",
    "images = load_images_from_folder(image_folder)\n",
    "print(\"Loaded image shape:\", images.shape)  # 이미지 shape 출력 : (n, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi9iaifLrOQr"
   },
   "source": [
    "# Autoencoder 정의 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRvoTXMcozlY"
   },
   "outputs": [],
   "source": [
    "def build_autoencoder():\n",
    "    # 인코더\n",
    "    encoder_input = layers.Input(shape=(256, 256, 1))\n",
    "    x = layers.Conv2D(32, 3, strides=2, activation='relu', padding='same')(encoder_input)\n",
    "    x = layers.Conv2D(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    encoded = layers.Dense(100)(x)\n",
    "\n",
    "    # 디코더\n",
    "    x = layers.Dense(64 * 64 * 64, activation='relu')(encoded)\n",
    "    x = layers.Reshape((64, 64, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    decoder_output = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "    encoder = models.Model(encoder_input, encoded)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "autoencoder, encoder = build_autoencoder() # 모델 생\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clblDiqPPHjq"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "autoencoder.fit(images, images, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMTpCzlOrQ8b"
   },
   "source": [
    "# 이미지 → 벡터 인코딩 및 시계열 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKndftm9o02a"
   },
   "outputs": [],
   "source": [
    "latents = encoder.predict(images) # 인코더를 통해 잠재 변수(latent) 추출\n",
    "\n",
    "# 잠재 변수들을 시계열 데이터로 변환\n",
    "def create_sequences(data, seq_len=12):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        sequences.append(data[i:i+seq_len])\n",
    "    return np.array(sequences)\n",
    "\n",
    "seq_data = create_sequences(latents, seq_len=12)\n",
    "print(\"TimeGAN input shape:\", seq_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrN2TfkirVTC"
   },
   "source": [
    "# LSTM Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftc8iU5zo2X1"
   },
   "outputs": [],
   "source": [
    "# LSTM Generator 모델\n",
    "def build_lstm_generator(seq_len=12, latent_dim=100):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(seq_len, latent_dim)),\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.LSTM(100, return_sequences=False),\n",
    "        layers.Dense(latent_dim)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm_gen = build_lstm_generator()\n",
    "X = seq_data[:, :-1, :] # 입력 시퀀스\n",
    "y = seq_data[:, -1, :] # 타겟 값\n",
    "lstm_gen.fit(X, y, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kT61a2Iryi6"
   },
   "source": [
    "# 생성된 벡터 → 이미지 디코딩 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRPv9hNio3rF"
   },
   "outputs": [],
   "source": [
    "pred_latent = lstm_gen.predict(X) # 예측된 잠재 변수\n",
    "\n",
    "# 디코더 모 정의\n",
    "decoder_input = layers.Input(shape=(100,))\n",
    "x = layers.Dense(64 * 64 * 64, activation='relu')(decoder_input)\n",
    "x = layers.Reshape((64, 64, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "decoder = models.Model(decoder_input, decoder_output)\n",
    "\n",
    "# 오토인코더의 가중치를 디코더에 전달\n",
    "for i in range(len(decoder.layers)):\n",
    "    try:\n",
    "        decoder.layers[i].set_weights(autoencoder.layers[-(len(decoder.layers)-i)].get_weights())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "reconstructed_imgs = decoder.predict(pred_latent)\n",
    "print(reconstructed_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-HZm0KDr0tB"
   },
   "source": [
    "# 이미지 복원 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jywKlpGXCOfe"
   },
   "outputs": [],
   "source": [
    "# MSE (Mean Squared Error) 계산\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "index = 0\n",
    "real_image = images[index+12].squeeze()  # 실제 이미지\n",
    "predicted_image = reconstructed_imgs[index].squeeze()  # 예측 이미지\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(real_image.flatten(), predicted_image.flatten())\n",
    "\n",
    "max_pixel_value = 1.0\n",
    "difference_percentage = mse / (max_pixel_value ** 2) * 100\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(images[index+12].squeeze(), cmap='gray')  # 실제\n",
    "plt.title('Real Next Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_imgs[index].squeeze(), cmap='gray')  # 예측\n",
    "plt.title(f'Predicted Image \\n Difference Percentage:{difference_percentage:.2f}%')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPAnFuMy7cMm9v2JGBX40GY",
   "gpuType": "T4",
   "mount_file_id": "1SCya2PmqrbHEyqezxhFNVnqSDHwU3gnX",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
