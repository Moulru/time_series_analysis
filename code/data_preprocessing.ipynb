{"cells":[{"cell_type":"code","source":["# Department: ESTSOFT\n","# Class: AI Modelling\n","# Category: Machine learning\n","# Title: Ice Extent Prediction\n","# Contributors: Kimm Soo Min\n","# Last modified date: 09/05/25"],"metadata":{"id":"MxbTY0rNPFgq"},"id":"MxbTY0rNPFgq","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"43b0d019","metadata":{"id":"43b0d019"},"outputs":[],"source":["# Library\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from PIL import Image, ImageOps\n","import re\n","from datetime import datetime\n","import glob\n","import os"]},{"cell_type":"code","execution_count":null,"id":"8b7d2361","metadata":{"id":"8b7d2361"},"outputs":[],"source":["# File path\n","DIR_PATH = \"/content/drive/MyDrive/Class/ESTSOFT/project/time_series_analysis/data/southern_hemisphere\"\n","pattern = \"image_orig/S_*_extn_hires_v3.0.png\"\n","input_path = os.path.join(DIR_PATH, pattern)\n","input_list = sorted(glob.glob(input_path))"]},{"cell_type":"code","execution_count":null,"id":"0565cfcd","metadata":{"id":"0565cfcd"},"outputs":[],"source":["# Image processing\n","for input in input_list:\n","\t# Input\n","\timg = Image.open(input)\n","\tinput_name = os.path.basename(input)\n","\tdate_match = re.search(r'(\\d{6})', input_name)\n","\tdate = datetime.strptime(date_match.group(1), \"%Y%m\") if date_match else None\n","\n","\t# Crop\n","\tcrop_box = (100, 305, 1140, 1405)\n","\timg_cropped = img.crop((crop_box))\n","\n","\t# Pad to square\n","\twidth, height = img_cropped.size\n","\tmax_side = max(width, height)\n","\tpad_left = (max_side - width) // 2\n","\tpad_top = (max_side - height) // 2\n","\tpad_right = max_side - width - pad_left\n","\tpad_bottom = max_side - height - pad_top\n","\tpad_colour = img_cropped.getpixel((50, 50))\n","\timg_padded = ImageOps.expand(img_cropped, border=(pad_left, pad_top, pad_right, pad_bottom), fill=pad_colour)\n","\n","\t# Resize\n","\timg_resized = img_padded.resize((1024, 1024), resample=Image.LANCZOS)\n","\n","\t# Save\n","\tos.makedirs(\"image_resized\", exist_ok=True)\n","\toutput_name = \"image_resized/img_resized_\" + date.strftime(\"%Y%m\") + \".png\"\n","\toutput_path = os.path.join(DIR_PATH, output_name)\n","\tif os.path.exists(output_path): # Skip if already exists\n","\t\tprint(f\"Skipping {output_name}, already exists.\")\n","\t\tcontinue\n","\timg_resized.save(output_path)"]},{"cell_type":"code","source":["# File path\n","pattern = \"image_resized/img_resized_*.png\"\n","input_path = os.path.join(DIR_PATH, pattern)\n","input_list = sorted(glob.glob(input_path))"],"metadata":{"id":"w35boKw8PYeK"},"id":"w35boKw8PYeK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to greyscale\n","for input in input_list:\n","    # Input\n","\timg = Image.open(input)\n","\tinput_name = os.path.basename(input)\n","\tdate_match = re.search(r'(\\d{6})', input_name)\n","\tdate = datetime.strptime(date_match.group(1), \"%Y%m\") if date_match else None\n","\n","    # Convert\n","\timg_grey = img.convert('L')\n","\timg_grey = img_grey.resize((256, 256), resample=Image.LANCZOS)\n","\n","    # Save\n","\tos.makedirs(\"image_grey/256x256\", exist_ok=True)\n","\toutput_name = \"image_grey/256x256/img_grey_\" + date.strftime(\"%Y%m\") + \".png\"\n","\toutput_path = os.path.join(DIR_PATH, output_name)\n","\tif os.path.exists(output_path): # Skip if already exists\n","\t\tprint(f\"Skipping {output_name}, already exists.\")\n","\t\tcontinue\n","\timg_grey.save(output_path)"],"metadata":{"id":"d0KBjQU6Pa8e"},"id":"d0KBjQU6Pa8e","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File path\n","pattern = \"ice_extent/S_??_extent_v3.0.csv\"\n","input_path = os.path.join(DIR_PATH, pattern)\n","input_list = sorted(glob.glob(output_path))"],"metadata":{"id":"ObGkP4hzPdRF"},"id":"ObGkP4hzPdRF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read files\n","dfs = []\n","for input in input_list:\n","    df = pd.read_csv(input, skipinitialspace=True)\n","    dfs.append(df)\n","df_merged = pd.concat(dfs, ignore_index=True)\n","\n","# Create date column\n","df_merged['date'] = pd.to_datetime(df_merged['year'].astype(str) + df_merged['mo'].astype(str), format='%Y%m')\n","df_merged.sort_values('date', inplace=True)\n","df_merged.drop(columns=['year', 'mo'], inplace=True)\n","\n","# Save\n","output_path = os.path.join(DIR_PATH, \"ice_extent/S_extent_merged.csv\")\n","df_merged.to_csv(output_path, index=False)"],"metadata":{"id":"GGjVRpmtPfPj"},"id":"GGjVRpmtPfPj","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}